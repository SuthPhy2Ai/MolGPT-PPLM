# MolGPT + EDL è®­ç»ƒæŒ‡å—

## ç›®å½•
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è®­ç»ƒè„šæœ¬ä½¿ç”¨](#è®­ç»ƒè„šæœ¬ä½¿ç”¨)
- [å‚æ•°è¯¦ç»†è¯´æ˜](#å‚æ•°è¯¦ç»†è¯´æ˜)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [è¾“å‡ºæ–‡ä»¶è¯´æ˜](#è¾“å‡ºæ–‡ä»¶è¯´æ˜)
- [EDLæ¦‚å¿µè¯´æ˜](#edlæ¦‚å¿µè¯´æ˜)
- [å¤šGPUè®­ç»ƒ](#å¤šgpuè®­ç»ƒ)
- [ç›‘æ§å’Œè°ƒè¯•](#ç›‘æ§å’Œè°ƒè¯•)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [ç¤ºä¾‹å‘½ä»¤](#ç¤ºä¾‹å‘½ä»¤)

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²æ¿€æ´»æ­£ç¡®çš„condaç¯å¢ƒï¼š
```bash
conda activate molgpt_dos
```

### 2. ä¸€é”®å¯åŠ¨è®­ç»ƒ

ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆ8 GPUï¼ŒEDLæ¨¡å¼ï¼‰ï¼š
```bash
cd /data/home/hzw1010/suth/edl_transformer/molgpt/train
./train_edl.sh
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/edl_molgpt_*.log

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### 4. åœæ­¢è®­ç»ƒ

```bash
kill $(cat train.pid)
```

---

## è®­ç»ƒè„šæœ¬ä½¿ç”¨

### åŸºæœ¬ç”¨æ³•

```bash
./train_edl.sh [OPTIONS]
```

### å¸¸ç”¨é€‰é¡¹

| é€‰é¡¹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--run_name NAME` | å®éªŒåç§° | `edl_molgpt_YYYYMMDD_HHMMSS` |
| `--max_epochs N` | è®­ç»ƒè½®æ•° | 20 |
| `--batch_size N` | æ‰¹é‡å¤§å° | 512 |
| `--learning_rate LR` | å­¦ä¹ ç‡ | 6e-4 |
| `--num_gpus N` | GPUæ•°é‡ | 8 |
| `--gpu_ids IDS` | GPU IDåˆ—è¡¨ | "0,1,2,3,4,5,6,7" |

### ç¤ºä¾‹

```bash
# ä½¿ç”¨4ä¸ªGPUè®­ç»ƒ
./train_edl.sh --num_gpus 4 --gpu_ids "0,1,2,3"

# è‡ªå®šä¹‰å®éªŒåç§°å’Œå‚æ•°
./train_edl.sh --run_name my_experiment --max_epochs 50 --batch_size 256

# å•GPUè®­ç»ƒ
./train_edl.sh --num_gpus 1 --gpu_ids "0"
```

---

## å‚æ•°è¯¦ç»†è¯´æ˜

### è®­ç»ƒå‚æ•°

#### `--run_name` (å­—ç¬¦ä¸²)
- **è¯´æ˜**: å®éªŒè¿è¡Œåç§°ï¼Œç”¨äºæ ‡è¯†ä¸åŒçš„è®­ç»ƒä»»åŠ¡
- **é»˜è®¤å€¼**: `edl_molgpt_YYYYMMDD_HHMMSS` (è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³)
- **ç”¨é€”**:
  - æ—¥å¿—æ–‡ä»¶å: `logs/{run_name}.log`
  - æ¨¡å‹ä¿å­˜è·¯å¾„: `../cond_gpt/weights/{run_name}.pt`
  - W&Bè¿è¡Œåç§°

#### `--max_epochs` (æ•´æ•°)
- **è¯´æ˜**: è®­ç»ƒçš„æ€»è½®æ•°
- **é»˜è®¤å€¼**: 20
- **å»ºè®®å€¼**:
  - å¿«é€Ÿæµ‹è¯•: 1-5
  - æ­£å¸¸è®­ç»ƒ: 20-50
  - å®Œæ•´è®­ç»ƒ: 50-100

#### `--batch_size` (æ•´æ•°)
- **è¯´æ˜**: æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°
- **é»˜è®¤å€¼**: 512
- **æ³¨æ„**:
  - å®é™…æ€»æ‰¹é‡ = batch_size Ã— num_gpus
  - 8 GPU Ã— 512 = 4096 (æœ‰æ•ˆæ‰¹é‡)
  - æ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼Œå¦‚æœOOMåˆ™å‡å°

#### `--learning_rate` (æµ®ç‚¹æ•°)
- **è¯´æ˜**: åˆå§‹å­¦ä¹ ç‡
- **é»˜è®¤å€¼**: 6e-4
- **å»ºè®®å€¼**:
  - å°æ¨¡å‹: 1e-3 ~ 1e-4
  - å¤§æ¨¡å‹: 1e-4 ~ 1e-5
  - ä½¿ç”¨warmupå’Œcosine decay

### æ¨¡å‹æ¶æ„å‚æ•°

#### `--n_layer` (æ•´æ•°)
- **è¯´æ˜**: Transformerå±‚æ•°
- **é»˜è®¤å€¼**: 8
- **å½±å“**: æ¨¡å‹å®¹é‡å’Œè®­ç»ƒæ—¶é—´
- **å»ºè®®å€¼**: 6-12

#### `--n_head` (æ•´æ•°)
- **è¯´æ˜**: æ³¨æ„åŠ›å¤´æ•°
- **é»˜è®¤å€¼**: 8
- **çº¦æŸ**: å¿…é¡»èƒ½è¢« n_embd æ•´é™¤

#### `--n_embd` (æ•´æ•°)
- **è¯´æ˜**: åµŒå…¥ç»´åº¦
- **é»˜è®¤å€¼**: 256
- **å»ºè®®å€¼**: 128, 256, 512, 768

### EDLå‚æ•°

#### `--use_edl` (æ ‡å¿—)
- **è¯´æ˜**: å¯ç”¨Evidential Deep Learningæ¨¡å¼
- **é»˜è®¤å€¼**: true
- **æ•ˆæœ**:
  - å¯ç”¨: è¾“å‡ºä¸ç¡®å®šæ€§é‡åŒ–
  - ç¦ç”¨: æ ‡å‡†softmaxåˆ†ç±»

#### `--edl_loss_type` (å­—ç¬¦ä¸²)
- **è¯´æ˜**: EDLæŸå¤±å‡½æ•°ç±»å‹
- **é€‰é¡¹**:
  - `mse`: Type II Maximum Likelihood (æ¨è)
  - `log`: äº¤å‰ç†µç‰ˆæœ¬
- **é»˜è®¤å€¼**: mse

#### `--edl_annealing_step` (æ•´æ•°)
- **è¯´æ˜**: KLæ•£åº¦é€€ç«æ­¥æ•°ï¼ˆepochï¼‰
- **é»˜è®¤å€¼**: 10
- **ä½œç”¨**: å‰Nä¸ªepoché€æ­¥å¢åŠ KLæƒé‡ï¼Œé¿å…è¿‡æ—©æ”¶æ•›

#### `--no_edl` (æ ‡å¿—)
- **è¯´æ˜**: ç¦ç”¨EDLæ¨¡å¼ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒ
- **ç”¨æ³•**: `./train_edl.sh --no_edl`

### GPUé…ç½®å‚æ•°

#### `--num_gpus` (æ•´æ•°)
- **è¯´æ˜**: ä½¿ç”¨çš„GPUæ•°é‡
- **é»˜è®¤å€¼**: 8
- **èŒƒå›´**: 1-8 (å–å†³äºå¯ç”¨GPU)

#### `--gpu_ids` (å­—ç¬¦ä¸²)
- **è¯´æ˜**: ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
- **é»˜è®¤å€¼**: "0,1,2,3,4,5,6,7"
- **ç¤ºä¾‹**:
  - ä½¿ç”¨GPU 0,1: `--gpu_ids "0,1"`
  - ä½¿ç”¨GPU 2,3,4: `--gpu_ids "2,3,4"`

### æ•°æ®å‚æ•°

#### `--data_name` (å­—ç¬¦ä¸²)
- **è¯´æ˜**: æ•°æ®é›†åç§°
- **é»˜è®¤å€¼**: moses2
- **ä½ç½®**: `../datasets/{data_name}/`

### å…¶ä»–å‚æ•°

#### `--wandb_online` (æ ‡å¿—)
- **è¯´æ˜**: å¯ç”¨åœ¨çº¿W&Bæ—¥å¿—è®°å½•
- **é»˜è®¤å€¼**: offline
- **ç”¨æ³•**: `./train_edl.sh --wandb_online`

---

## ç›®å½•ç»“æ„

```
molgpt/
â”œâ”€â”€ train/                          # è®­ç»ƒè„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ train_edl.sh               # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train.py                   # Pythonè®­ç»ƒå…¥å£
â”‚   â”œâ”€â”€ trainer.py                 # Trainerç±»å®ç°
â”‚   â”œâ”€â”€ model.py                   # GPTæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ edl_loss.py               # EDLæŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ uncertainty.py            # ä¸ç¡®å®šæ€§è®¡ç®—
â”‚   â”œâ”€â”€ utils.py                  # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ dataset.py                # æ•°æ®é›†ç±»
â”‚   â”œâ”€â”€ test_edl.py               # EDLå•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ guide.md                  # æœ¬æŒ‡å—
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                     # è®­ç»ƒæ—¥å¿—ç›®å½•
â”‚   â”‚   â””â”€â”€ {run_name}.log       # è®­ç»ƒæ—¥å¿—æ–‡ä»¶
â”‚   â”‚
â”‚   â”œâ”€â”€ wandb/                    # W&Bæ—¥å¿—ç›®å½•
â”‚   â”‚   â””â”€â”€ offline-run-*/       # ç¦»çº¿è¿è¡Œæ•°æ®
â”‚   â”‚
â”‚   â”œâ”€â”€ train.pid                 # è®­ç»ƒè¿›ç¨‹PIDæ–‡ä»¶
â”‚   â””â”€â”€ datasets -> ../datasets/  # æ•°æ®é›†ç¬¦å·é“¾æ¥
â”‚
â”œâ”€â”€ cond_gpt/
â”‚   â””â”€â”€ weights/                  # æ¨¡å‹æƒé‡ç›®å½•
â”‚       â””â”€â”€ {run_name}.pt        # è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚
â”œâ”€â”€ datasets/                     # æ•°æ®é›†ç›®å½•
â”‚   â””â”€â”€ moses2/                  # MOSESæ•°æ®é›†
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ generate/                     # ç”Ÿæˆè„šæœ¬ç›®å½•
    â””â”€â”€ utils.py                 # ç”Ÿæˆå·¥å…·å‡½æ•°
```

---

## è¾“å‡ºæ–‡ä»¶è¯´æ˜

### 1. è®­ç»ƒæ—¥å¿— (`logs/{run_name}.log`)

**ä½ç½®**: `molgpt/train/logs/`

**å†…å®¹**:
- è®­ç»ƒé…ç½®ä¿¡æ¯
- æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±
- å­¦ä¹ ç‡å˜åŒ–
- æ¨¡å‹ä¿å­˜ä¿¡æ¯
- é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯

**ç¤ºä¾‹**:
```
epoch 1 iter 100: train loss 0.9876. lr 1.234e-04
test loss: 0.9543
Saving at epoch 1
```

### 2. æ¨¡å‹æƒé‡ (`{run_name}.pt`)

**ä½ç½®**: `molgpt/cond_gpt/weights/`

**å†…å®¹**: PyTorchæ¨¡å‹state_dict

**å¤§å°**: çº¦25-100MBï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰

**åŠ è½½æ–¹å¼**:
```python
import torch
from model import GPT, GPTConfig

# åŠ è½½checkpoint
checkpoint = torch.load('weights/my_model.pt')

# åˆ›å»ºæ¨¡å‹
config = GPTConfig(vocab_size=64, block_size=72,
                   n_layer=8, n_head=8, n_embd=256,
                   use_edl=True)
model = GPT(config)

# åŠ è½½æƒé‡
model.load_state_dict(checkpoint)
```

### 3. W&Bæ—¥å¿— (`wandb/offline-run-*/`)

**ä½ç½®**: `molgpt/train/wandb/`

**å†…å®¹**:
- è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿
- å­¦ä¹ ç‡æ›²çº¿
- ç³»ç»ŸæŒ‡æ ‡ï¼ˆGPUä½¿ç”¨ç‡ã€å†…å­˜ç­‰ï¼‰

**åŒæ­¥åˆ°äº‘ç«¯**:
```bash
wandb sync wandb/offline-run-XXXXXXXX-XXXXXXXX
```

### 4. PIDæ–‡ä»¶ (`train.pid`)

**ä½ç½®**: `molgpt/train/`

**å†…å®¹**: è®­ç»ƒè¿›ç¨‹çš„PID

**ç”¨é€”**: ç”¨äºåœæ­¢è®­ç»ƒ
```bash
kill $(cat train.pid)
```

---

## EDLæ¦‚å¿µè¯´æ˜

### ä»€ä¹ˆæ˜¯Evidential Deep Learning (EDL)?

EDLæ˜¯ä¸€ç§ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡Dirichletåˆ†å¸ƒå»ºæ¨¡åˆ†ç±»æ¦‚ç‡çš„ä¸ç¡®å®šæ€§ã€‚

### ä¼ ç»Ÿæ–¹æ³• vs EDL

| æ–¹é¢ | ä¼ ç»ŸSoftmax | EDL |
|------|-------------|-----|
| è¾“å‡º | æ¦‚ç‡åˆ†å¸ƒï¼ˆç‚¹ä¼°è®¡ï¼‰ | æ¦‚ç‡åˆ†å¸ƒ + ä¸ç¡®å®šæ€§ |
| æŸå¤±å‡½æ•° | äº¤å‰ç†µ | Type II ML + KLæ•£åº¦ |
| ä¸ç¡®å®šæ€§ | æ— æ³•é‡åŒ– | åˆ†è§£ä¸ºè®¤çŸ¥å’Œå¶ç„¶ |

### ä¸ç¡®å®šæ€§ç±»å‹

#### 1. æ€»ä¸ç¡®å®šæ€§ (Total Uncertainty)
- **å®šä¹‰**: u = K / S
- **å«ä¹‰**: æ¨¡å‹å¯¹é¢„æµ‹çš„æ•´ä½“ä¸ç¡®å®šç¨‹åº¦
- **èŒƒå›´**: [0, âˆ)ï¼Œè¶Šå°è¶Šç¡®å®š

#### 2. è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)
- **å®šä¹‰**: äº’ä¿¡æ¯ I[y, Î¸|D]
- **å«ä¹‰**: æ¨¡å‹å‚æ•°çš„ä¸ç¡®å®šæ€§ï¼ˆå¯é€šè¿‡æ›´å¤šæ•°æ®å‡å°‘ï¼‰
- **åº”ç”¨**: ä¸»åŠ¨å­¦ä¹ ã€æ•°æ®é€‰æ‹©

#### 3. å¶ç„¶ä¸ç¡®å®šæ€§ (Aleatoric Uncertainty)
- **å®šä¹‰**: æœŸæœ›ç†µ E[H[p]]
- **å«ä¹‰**: æ•°æ®æœ¬èº«çš„å™ªå£°ï¼ˆä¸å¯å‡å°‘ï¼‰
- **åº”ç”¨**: é£é™©è¯„ä¼°ã€è´¨é‡æ§åˆ¶

### EDLè®­ç»ƒæµç¨‹

```
è¾“å…¥ â†’ Transformer â†’ çº¿æ€§å±‚ â†’ Softplus â†’ Evidence
                                              â†“
                                         Î± = Evidence + 1
                                              â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â†“                   â†“
                              é¢„æµ‹æ¦‚ç‡              ä¸ç¡®å®šæ€§
                            p = Î± / S          u = K / S
```

### KL Annealing

**ç›®çš„**: é¿å…æ¨¡å‹è¿‡æ—©æ”¶æ•›åˆ°ä½ä¸ç¡®å®šæ€§çŠ¶æ€

**ç­–ç•¥**:
```python
if epoch < annealing_step:
    kl_weight = epoch / annealing_step
else:
    kl_weight = 1.0

loss = mse_loss + kl_weight * kl_divergence
```

**æ•ˆæœ**:
- å‰æœŸï¼ˆepoch < 10ï¼‰: ä¸“æ³¨äºæ‹Ÿåˆæ•°æ®
- åæœŸï¼ˆepoch â‰¥ 10ï¼‰: å¹³è¡¡æ‹Ÿåˆå’Œä¸ç¡®å®šæ€§

---

## å¤šGPUè®­ç»ƒ

### Accelerateåº“

æœ¬é¡¹ç›®ä½¿ç”¨Hugging Face Accelerateåº“å®ç°å¤šGPUè®­ç»ƒã€‚

**ä¼˜åŠ¿**:
- è‡ªåŠ¨å¤„ç†æ•°æ®å¹¶è¡Œ
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
- ç®€å•çš„APIï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç 
- æ”¯æŒå•GPUã€å¤šGPUã€TPU

### è®­ç»ƒæ¨¡å¼

#### å•GPUè®­ç»ƒ
```bash
./train_edl.sh --num_gpus 1 --gpu_ids "0"
```

#### å¤šGPUè®­ç»ƒï¼ˆæ•°æ®å¹¶è¡Œï¼‰
```bash
./train_edl.sh --num_gpus 8 --gpu_ids "0,1,2,3,4,5,6,7"
```

### æœ‰æ•ˆæ‰¹é‡å¤§å°

```
æœ‰æ•ˆæ‰¹é‡ = batch_size Ã— num_gpus Ã— gradient_accumulation_steps
```

**ç¤ºä¾‹**:
- å•GPU: 512 Ã— 1 = 512
- 8 GPU: 512 Ã— 8 = 4096

### æ€§èƒ½ä¼˜åŒ–

#### 1. æ··åˆç²¾åº¦è®­ç»ƒ
- è‡ªåŠ¨å¯ç”¨FP16ï¼ˆå¦‚æœGPUæ”¯æŒï¼‰
- åŠ é€Ÿè®­ç»ƒçº¦1.5-2x
- å‡å°‘æ˜¾å­˜ä½¿ç”¨çº¦50%

#### 2. æ•°æ®åŠ è½½
- `num_workers=10`: å¹¶è¡Œæ•°æ®åŠ è½½
- `pin_memory=True`: åŠ é€ŸCPUåˆ°GPUä¼ è¾“

#### 3. æ¢¯åº¦ç´¯ç§¯
- å½“GPUå†…å­˜ä¸è¶³æ—¶ä½¿ç”¨
- æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡å¤§å°

---

## ç›‘æ§å’Œè°ƒè¯•

### å®æ—¶ç›‘æ§

#### 1. è®­ç»ƒæ—¥å¿—
```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f logs/{run_name}.log

# æŸ¥çœ‹æœ€è¿‘100è¡Œ
tail -100 logs/{run_name}.log

# æœç´¢é”™è¯¯
grep -i "error\|failed" logs/{run_name}.log
```

#### 2. GPUä½¿ç”¨æƒ…å†µ
```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹ç‰¹å®šGPU
nvidia-smi -i 0,1,2,3
```

#### 3. è¿›ç¨‹çŠ¶æ€
```bash
# æŸ¥çœ‹è®­ç»ƒè¿›ç¨‹
ps aux | grep train.py

# æŸ¥çœ‹PID
cat train.pid
```

### è®­ç»ƒæŒ‡æ ‡

#### æŸå¤±æ›²çº¿
- **è®­ç»ƒæŸå¤±**: åº”è¯¥æŒç»­ä¸‹é™
- **éªŒè¯æŸå¤±**: åº”è¯¥ä¸‹é™ï¼Œæ³¨æ„è¿‡æ‹Ÿåˆ
- **æ­£å¸¸èŒƒå›´**: 0.5-1.5ï¼ˆå–å†³äºæ•°æ®é›†ï¼‰

#### å­¦ä¹ ç‡
- **Warmupé˜¶æ®µ**: çº¿æ€§å¢é•¿
- **è®­ç»ƒé˜¶æ®µ**: Cosineè¡°å‡
- **æœ€ç»ˆå€¼**: çº¦ä¸ºåˆå§‹å€¼çš„10%

### è°ƒè¯•æŠ€å·§

#### 1. å¿«é€Ÿæµ‹è¯•
```bash
# 1ä¸ªepochï¼Œå°æ‰¹é‡
./train_edl.sh --run_name test --max_epochs 1 --batch_size 64
```

#### 2. å•å…ƒæµ‹è¯•
```bash
# è¿è¡ŒEDLæµ‹è¯•
python test_edl.py
```

#### 3. æ£€æŸ¥æ•°æ®
```python
from dataset import MoleculeDataset

dataset = MoleculeDataset('moses2', 'train')
print(f"Dataset size: {len(dataset)}")
print(f"Vocab size: {dataset.vocab_size}")
print(f"Max length: {dataset.max_len}")
```

---

## å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶å‡ºç° "CUDA out of memory" é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch_size: `--batch_size 256`
2. å‡å°æ¨¡å‹å¤§å°: `--n_layer 6 --n_embd 128`
3. ä½¿ç”¨æ›´å°‘çš„GPU: `--num_gpus 4`

### Q2: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

**å¯èƒ½åŸå› **:
1. æ•°æ®åŠ è½½ç“¶é¢ˆ â†’ å¢åŠ num_workers
2. GPUåˆ©ç”¨ç‡ä½ â†’ å¢å¤§batch_size
3. æ··åˆç²¾åº¦æœªå¯ç”¨ â†’ æ£€æŸ¥GPUæ˜¯å¦æ”¯æŒFP16

### Q3: æŸå¤±ä¸ä¸‹é™æˆ–å‡ºç°NaN

**æ£€æŸ¥é¡¹**:
1. å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§ â†’ é™ä½learning_rate
2. æ¢¯åº¦çˆ†ç‚¸ â†’ æ£€æŸ¥grad_norm_clipè®¾ç½®
3. æ•°æ®é—®é¢˜ â†’ æ£€æŸ¥æ•°æ®é¢„å¤„ç†

### Q4: å¦‚ä½•æ¢å¤ä¸­æ–­çš„è®­ç»ƒï¼Ÿ

**å½“å‰ä¸æ”¯æŒè‡ªåŠ¨æ¢å¤**ï¼Œéœ€è¦æ‰‹åŠ¨å®ç°ï¼š
```python
# åœ¨train.pyä¸­æ·»åŠ 
if os.path.exists(checkpoint_path):
    model.load_state_dict(torch.load(checkpoint_path))
```

### Q5: EDLæ¨¡å¼å’Œæ ‡å‡†æ¨¡å¼æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

| ç‰¹æ€§ | æ ‡å‡†æ¨¡å¼ | EDLæ¨¡å¼ |
|------|---------|---------|
| è¾“å‡º | Logits | Evidence (Î±) |
| æŸå¤± | CrossEntropy | EDL MSE/Log |
| ä¸ç¡®å®šæ€§ | æ—  | æœ‰ï¼ˆ3ç§ï¼‰ |
| è®­ç»ƒæ—¶é—´ | å¿« | ç¨æ…¢ï¼ˆ+10-20%ï¼‰ |

### Q6: å¦‚ä½•é€‰æ‹©EDLå‚æ•°ï¼Ÿ

**edl_loss_type**:
- `mse`: æ›´ç¨³å®šï¼Œæ¨èç”¨äºå¤§å¤šæ•°æƒ…å†µ
- `log`: å¯èƒ½æ”¶æ•›æ›´å¿«ï¼Œä½†ä¸å¤ªç¨³å®š

**edl_annealing_step**:
- å°æ•°æ®é›†: 5-10 epochs
- å¤§æ•°æ®é›†: 10-20 epochs
- è§„åˆ™: çº¦ä¸ºæ€»epochæ•°çš„20-50%

---

## ç¤ºä¾‹å‘½ä»¤

### 1. å¿«é€Ÿæµ‹è¯•ï¼ˆ1 epochï¼Œå•GPUï¼‰
```bash
./train_edl.sh \
    --run_name quick_test \
    --max_epochs 1 \
    --batch_size 64 \
    --num_gpus 1 \
    --gpu_ids "0"
```

### 2. æ ‡å‡†è®­ç»ƒï¼ˆ8 GPUï¼ŒEDLï¼‰
```bash
./train_edl.sh \
    --run_name standard_edl \
    --max_epochs 20 \
    --batch_size 512 \
    --num_gpus 8 \
    --gpu_ids "0,1,2,3,4,5,6,7"
```

### 3. å¤§æ¨¡å‹è®­ç»ƒ
```bash
./train_edl.sh \
    --run_name large_model \
    --max_epochs 50 \
    --batch_size 256 \
    --n_layer 12 \
    --n_head 12 \
    --n_embd 512 \
    --num_gpus 8
```

### 4. æ ‡å‡†æ¨¡å¼è®­ç»ƒï¼ˆæ— EDLï¼‰
```bash
./train_edl.sh \
    --run_name baseline \
    --max_epochs 20 \
    --no_edl \
    --num_gpus 4 \
    --gpu_ids "0,1,2,3"
```

### 5. è‡ªå®šä¹‰EDLå‚æ•°
```bash
./train_edl.sh \
    --run_name custom_edl \
    --max_epochs 30 \
    --edl_loss_type log \
    --edl_annealing_step 15 \
    --num_gpus 8
```

### 6. åœ¨çº¿W&Bæ—¥å¿—
```bash
./train_edl.sh \
    --run_name wandb_online \
    --max_epochs 20 \
    --wandb_online \
    --num_gpus 8
```

---

## é™„å½•

### A. ç¯å¢ƒè¦æ±‚

- Python: 3.8+
- PyTorch: 2.0+
- CUDA: 11.0+
- GPU: 8GB+ VRAM per GPU
- Accelerate: 0.20+

### B. æ•°æ®é›†æ ¼å¼

MOSESæ•°æ®é›†æ ¼å¼ï¼š
```csv
SMILES
CC(C)Cc1ccc(cc1)C(C)C(O)=O
CCCCc1cc(O)c(O)c(c1)C(=O)O
...
```

### C. æ¨¡å‹å‚æ•°é‡

| é…ç½® | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ |
|------|--------|----------|
| Small (6L, 128D) | ~5M | ~2GB |
| Medium (8L, 256D) | ~20M | ~4GB |
| Large (12L, 512D) | ~80M | ~8GB |

### D. æ€§èƒ½åŸºå‡†

**ç¡¬ä»¶**: 8Ã— NVIDIA A100 (40GB)

| é…ç½® | é€Ÿåº¦ | æ—¶é—´/Epoch |
|------|------|-----------|
| 1 GPU | ~6 it/s | ~4.3 min |
| 4 GPU | ~24 it/s | ~1.1 min |
| 8 GPU | ~45 it/s | ~35 sec |

---

## æ›´æ–°æ—¥å¿—

- **2026-01-24**: åˆå§‹ç‰ˆæœ¬
  - æ·»åŠ Accelerateå¤šGPUæ”¯æŒ
  - é›†æˆEDLä¸ç¡®å®šæ€§é‡åŒ–
  - ä¿®å¤train.pyçš„df.to_csv()bug

---

## è”ç³»å’Œæ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æœ¬æŒ‡å—çš„å¸¸è§é—®é¢˜éƒ¨åˆ†
2. è¿è¡Œå•å…ƒæµ‹è¯•: `python test_edl.py`
3. æ£€æŸ¥è®­ç»ƒæ—¥å¿—: `logs/{run_name}.log`

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€
